{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0d94a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI Agent\\code\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\AI Agent\\code\\.venv\\Lib\\site-packages\\llama_index\\llms\\gemini\\base.py:21: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from constants import embed_model \n",
    "\n",
    "# 1. Khôi phục lại \"Bộ nhớ\" (Index) đã lưu từ ổ cứng\n",
    "# StorageContext giúp định vị nơi chứa dữ liệu (folder \"index/\")\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"index/\")\n",
    "\n",
    "# 2. Tải Index lên bộ nhớ (RAM)\n",
    "# Cần truyền đúng embed_model đã dùng lúc tạo index (Gemini Embedding)\n",
    "index = load_index_from_storage(\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49df089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool \n",
    "from constants import llm_model\n",
    "\n",
    "# 3. Tạo \"Động cơ tìm kiếm\" (Query Engine)\n",
    "# Đây là bộ máy giúp tra cứu thông tin trong Index và dùng LLM để tổng hợp câu trả lời\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm_model,      # Dùng model Gemini 2.5 Flash để suy luận\n",
    "    similarity_top_k=5  # Lấy 5 đoạn văn bản liên quan nhất để đọc mỗi lần hỏi\n",
    ")\n",
    "\n",
    "# 4. Đóng gói thành Công cụ (Tool) cho Agent\n",
    "# Agent sẽ dùng tool này khi cần tra cứu các bài báo khoa học\n",
    "rag_engine = QueryEngineTool.from_defaults(\n",
    "    query_engine,\n",
    "    name=\"research_paper_query_engine_tool\", # Tên định danh của tool\n",
    "    description=\"A RAG engine with recent research papers.\", # Mô tả giúp Agent hiểu công dụng\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cd7c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown   \n",
    "\n",
    "# Hàm hỗ trợ hiển thị (Helper function)\n",
    "# Dùng để in các Prompt (câu lệnh ngầm) ra màn hình với định dạng Markdown đẹp mắt\n",
    "def display_prompt_dict(prompt_dict):\n",
    "    for key, prompt in prompt_dict.items():\n",
    "        display(Markdown(f\"**Prompt key:** {key}\")) # Tên loại prompt\n",
    "        print(prompt.get_template())                # Nội dung template của prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "675c4743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt key:** response_synthesizer:text_qa_template"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "---------------------\n",
      "{context_str}\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: {query_str}\n",
      "Answer: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Prompt key:** response_synthesizer:refine_template"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original query is as follows: {query_str}\n",
      "We have provided an existing answer: {existing_answer}\n",
      "We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "{context_msg}\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the query. If the context isn't useful, return the original answer.\n",
      "Refined Answer: \n"
     ]
    }
   ],
   "source": [
    "# 5. Xem \"Nội tạng\" các Prompt mặc định\n",
    "# Lệnh này giúp bạn kiểm tra xem LlamaIndex đang dùng câu lệnh gì để giao tiếp với LLM\n",
    "prompts_dict = query_engine.get_prompts()\n",
    "display_prompt_dict(prompts_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef727e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import download_pdf, fetch_arxiv_papers\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "# 6. Chuyển đổi hàm Python thường thành Tools cho Agent\n",
    "# FunctionTool giúp bọc các hàm mình tự viết lại để Agent có thể hiểu và gọi được\n",
    "\n",
    "# Tool giúp download file PDF từ link\n",
    "download_pdf_tool = FunctionTool.from_defaults(\n",
    "    download_pdf,\n",
    "    name=\"download_pdf_file_tool\",\n",
    "    description=\"python function that downloads a pdf file by link\",\n",
    ")\n",
    "\n",
    "# Tool giúp tìm kiếm các bài báo mới trên Arxiv\n",
    "fetch_arxiv_papers_tool = FunctionTool.from_defaults(\n",
    "    fetch_arxiv_papers,\n",
    "    name=\"fetch_from_arxiv\",\n",
    "    description=\"download the {max_results} recent research papers regarding the topic {title} from arXiv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bcc96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "# 7. Tạo bộ nhớ đệm (Chat Memory)\n",
    "# Vì Agent Workflow mặc định là \"không trạng thái\" (stateless), ta cần tự tạo bộ nhớ \n",
    "# để lưu lịch sử hội thoại, giúp Agent nhớ được các câu hỏi trước đó.\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=20000)\n",
    "\n",
    "# 8. Khởi tạo Agent ReAct (Reasoning + Acting)\n",
    "# Đây là \"bộ não\" chính, biết suy luận và quyết định dùng công cụ nào\n",
    "agent = ReActAgent(\n",
    "    tools=[download_pdf_tool, rag_engine, fetch_arxiv_papers_tool], # Danh sách \"vũ khí\" của Agent\n",
    "    llm=llm_model,     # Mô hình ngôn ngữ dùng để suy nghĩ\n",
    "    verbose=True,      # In ra quá trình suy nghĩ (Thinking process) để dễ debug\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a5a4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper \"ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools\" introduces ChatGLM as an evolving family of large language models. While the paper primarily focuses on the more recent GLM-4 series (GLM-4, GLM-4-Air, and GLM-4-9B), it positions GLM-130B as a foundational model within this lineage. The GLM-4 models were developed using insights and lessons gained from preceding generations of ChatGLM, which include GLM-130B. This indicates that GLM-130B was a significant step in the development of the ChatGLM family, contributing to the advancements seen in its successors. The paper highlights the continuous development and open-sourcing efforts of models within this family.\n"
     ]
    }
   ],
   "source": [
    "# 9. Chạy thử lần đầu: Tóm tắt bài báo về GLM-130B\n",
    "# Dùng await agent.run() thay vì chat() vì đây là Workflow Agent bất đồng bộ\n",
    "response = await agent.run(\n",
    "    user_msg=\"Summarize the paper about GLM-130B\", \n",
    "    max_iterations=10, # Giới hạn số bước suy luận tối đa để tránh vòng lặp vô tận\n",
    "    memory=memory      # Truyền bộ nhớ vào để cập nhật lịch sử chat\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fe7504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Định nghĩa Template cho câu hỏi phức tạp hơn\n",
    "# Hướng dẫn Agent cụ thể: Nếu tìm trong database có thì lấy, không có thì lên Arxiv tải về\n",
    "query_template = \"\"\" I am instered in {topic}.\n",
    " Find papers in your knowledge database related to this topic.\n",
    " Use the following template to query research_paper_query_engine_tool tool: 'Provide title,summary,authors and link to download for paper related to {topic}'. \n",
    " If there are not, could you fetch the recent one from arXiv?\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165fbe1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found the following paper related to \"Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models\":\n",
      "\n",
      "**Title:** Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models\n",
      "**Summary:** Humans develop internal world models for reasoning by manipulating concepts within them. While current AI systems, particularly those using chain-of-thought (CoT) reasoning, have achieved expert-level performance in formal domains like mathematics and programming through verbal reasoning, they struggle with physical and spatial intelligence, which demand richer representations and prior knowledge. The advent of unified multimodal models (UMMs) capable of both verbal and visual generation has led to interest in more human-like reasoning via multimodal pathways. This paper investigates when and how visual generation aids reasoning, proposing the visual superiority hypothesis: for tasks grounded in the physical world, visual generation more effectively serves as world models, overcoming the representational limitations or insufficient prior knowledge of purely verbal models. The work formalizes internal world modeling as a core part of CoT reasoning, analyzes different world model forms, and identifies tasks requiring interleaved visual-verbal CoT reasoning, introducing a new evaluation suite called VisWorld-Eval. Experiments with a state-of-the-art UMM demonstrate that interleaved CoT significantly outperforms purely verbal CoT on tasks that benefit from visual world modeling, but shows no clear advantage otherwise. This research clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.\n",
      "**Authors:** Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang, Changjing He, Chaoyi Deng, Renrui Zhang, Youbin Wu, Mingsheng Long\n",
      "**Link to download:** https://arxiv.org/pdf/2601.19834v1\n"
     ]
    }
   ],
   "source": [
    "# 11. Thử nghiệm với chủ đề \"Visual Generation...\"\n",
    "# Agent sẽ kiểm tra Index trước -> không có -> gọi tool fetch_arxiv_papers -> cập nhật Index -> trả lời\n",
    "answer = await agent.run(\n",
    "    user_msg=query_template.format(topic=\"Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models\"),\n",
    "    max_iterations=10,\n",
    "    memory=memory\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87257819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I found the following paper related to \"Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models\":\n",
       "\n",
       "**Title:** Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models\n",
       "**Summary:** Humans develop internal world models for reasoning by manipulating concepts within them. While current AI systems, particularly those using chain-of-thought (CoT) reasoning, have achieved expert-level performance in formal domains like mathematics and programming through verbal reasoning, they struggle with physical and spatial intelligence, which demand richer representations and prior knowledge. The advent of unified multimodal models (UMMs) capable of both verbal and visual generation has led to interest in more human-like reasoning via multimodal pathways. This paper investigates when and how visual generation aids reasoning, proposing the visual superiority hypothesis: for tasks grounded in the physical world, visual generation more effectively serves as world models, overcoming the representational limitations or insufficient prior knowledge of purely verbal models. The work formalizes internal world modeling as a core part of CoT reasoning, analyzes different world model forms, and identifies tasks requiring interleaved visual-verbal CoT reasoning, introducing a new evaluation suite called VisWorld-Eval. Experiments with a state-of-the-art UMM demonstrate that interleaved CoT significantly outperforms purely verbal CoT on tasks that benefit from visual world modeling, but shows no clear advantage otherwise. This research clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.\n",
       "**Authors:** Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang, Changjing He, Chaoyi Deng, Renrui Zhang, Youbin Wu, Mingsheng Long\n",
       "**Link to download:** https://arxiv.org/pdf/2601.19834v1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Hiển thị câu trả lời dưới dạng Markdown cho dễ đọc\n",
    "display(Markdown(str(answer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "996257ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found the following paper related to \"EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning\":\n",
      "\n",
      "**Title:** EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning\n",
      "**Authors:** Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu, Muzammal Naseer, Chi-Wing Fu, Pheng-Ann Heng\n",
      "**Summary:** Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. This paper introduces EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction designed to improve semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL incorporates complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D datasets demonstrate consistent improvements over state-of-the-art methods. The framework also shows real-world generalization and enhances EgoVLM hand-object interaction reasoning by utilizing reconstructed hands as visual prompts.\n",
      "**Link to download:** https://arxiv.org/pdf/2601.19850v1\n"
     ]
    }
   ],
   "source": [
    "answer = await agent.run(\n",
    "    user_msg=query_template.format(topic=\"EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning\"),\n",
    "    max_iterations=10,\n",
    "    memory=memory\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4596b34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I found the following paper related to \"EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning\":\n",
       "\n",
       "**Title:** EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning\n",
       "**Authors:** Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu, Muzammal Naseer, Chi-Wing Fu, Pheng-Ann Heng\n",
       "**Summary:** Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. This paper introduces EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction designed to improve semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL incorporates complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D datasets demonstrate consistent improvements over state-of-the-art methods. The framework also shows real-world generalization and enhances EgoVLM hand-object interaction reasoning by utilizing reconstructed hands as visual prompts.\n",
       "**Link to download:** https://arxiv.org/pdf/2601.19850v1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Hiển thị kết quả (Markdown mặc định căn trái)\n",
    "display(Markdown(str(answer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95c64edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have successfully downloaded both papers:\n",
      "1. \"Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models\"\n",
      "2. \"EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning\"\n"
     ]
    }
   ],
   "source": [
    "answer = await agent.run(\n",
    "    user_msg=\"Download all the papers you mentioned in previous turns.\",\n",
    "    max_iterations=10,\n",
    "    memory=memory\n",
    ")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
