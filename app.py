import streamlit as st
import asyncio
import nest_asyncio
from agent_class import Agent
from index_manager import IndexManager
from constants import GOOGLE_API_KEY
from llama_index.llms.gemini import Gemini
from llama_index.embeddings.gemini import GeminiEmbedding

from llama_index.core.memory import ChatMemoryBuffer

# Patch asyncio ƒë·ªÉ tr√°nh l·ªói khi ch·∫°y l·ªìng trong m√¥i tr∆∞·ªùng c√≥ s·∫µn event loop
nest_asyncio.apply()

st.set_page_config(page_title="Arxiv Research Agent", page_icon="üìö")
st.title("üìö Arxiv Research Agent")

# 1. Caching Resource cho Index (D·ªØ li·ªáu n·∫∑ng)
@st.cache_resource
def load_index_data():
    """Ch·ªâ load d·ªØ li·ªáu Index 1 l·∫ßn ƒë·ªÉ ti·∫øt ki·ªám RAM"""
    try:
        # D√πng gRPC m·∫∑c ƒë·ªãnh
        embed_model = GeminiEmbedding(
            api_key=GOOGLE_API_KEY, 
            model_name="models/gemini-embedding-001"
        )
        index_manager = IndexManager(embed_model)
        index = index_manager.retrieve_index()
        return index
    except Exception as e:
        print(f"Index load error: {e}")
        return None

def create_agent(index, memory):
    """T·∫°o Agent m·ªõi m·ªói l·∫ßn run ƒë·ªÉ g·∫Øn ƒë√∫ng Event Loop hi·ªán t·∫°i"""
    llm_model = Gemini(
        api_key=GOOGLE_API_KEY, 
        model_name="models/gemini-2.5-flash", 
        max_tokens=8192
    )
    # Truy·ªÅn memory t·ª´ session state v√†o
    return Agent(index, llm_model, memory=memory)

# 2. Kh·ªüi t·∫°o State ban ƒë·∫ßu
if "messages" not in st.session_state:
    st.session_state.messages = []

# L∆∞u tr·ªØ Chat Memory ri√™ng bi·ªát (kh√¥ng ph·ª• thu·ªôc Agent object)
if "chat_memory" not in st.session_state:
    st.session_state.chat_memory = ChatMemoryBuffer.from_defaults(token_limit=20000)

# 3. Load Index v√† T·∫°o Agent cho Run hi·ªán t·∫°i
index = load_index_data()

if index:
    # QUAN TR·ªåNG: Lu√¥n t·∫°o Agent m·ªõi cho m·ªói l·∫ßn ch·∫°y script
    # Nh∆∞ng d√πng l·∫°i b·ªô nh·ªõ c≈© (st.session_state.chat_memory)
    agent = create_agent(index, st.session_state.chat_memory)
else:
    st.error("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y Index! H√£y ch·∫°y file 'build_index.ipynb' ƒë·ªÉ t·∫°o d·ªØ li·ªáu tr∆∞·ªõc.")
    st.stop()

# 4. Hi·ªÉn th·ªã l·ªãch s·ª≠ chat UI
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# 5. X·ª≠ l√Ω Input
# C√∫ ph√°p walrus (:=) gi√∫p g√°n gi√° tr·ªã v√† ki·ªÉm tra ƒëi·ªÅu ki·ªán c√πng l√∫c
if prompt := st.chat_input("Ask me anything about research papers!"):
    # Hi·ªÉn th·ªã c√¢u h·ªèi User
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # X·ª≠ l√Ω c√¢u tr·∫£ l·ªùi Assistant
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            try:
                # L·∫•y event loop hi·ªán t·∫°i (ƒë√£ ƒë∆∞·ª£c patch b·ªüi nest_asyncio)
                loop = asyncio.get_event_loop()
                # Ch·∫°y task tr√™n loop hi·ªán t·∫°i
                answer_text = loop.run_until_complete(agent.chat(prompt))
                
                st.markdown(answer_text)
                
                # L∆∞u l·ªãch s·ª≠ UI
                st.session_state.messages.append({"role": "assistant", "content": answer_text})
            except Exception as e:
                st.error(f"Error: {str(e)}")
