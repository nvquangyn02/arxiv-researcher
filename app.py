import streamlit as st
import asyncio
import nest_asyncio
from agent_class import Agent
from index_manager import IndexManager
from constants import GOOGLE_API_KEY, embed_model
from llama_index.llms.gemini import Gemini
from llama_index.embeddings.gemini import GeminiEmbedding
from index_manager_pinecone import IndexManagerPinecone
from llama_index.core.memory import ChatMemoryBuffer

from index_manager_pinecone import IndexManagerPinecone

# Patch asyncio ƒë·ªÉ tr√°nh l·ªói khi ch·∫°y l·ªìng trong m√¥i tr∆∞·ªùng c√≥ s·∫µn event loop
nest_asyncio.apply()

st.set_page_config(page_title="Arxiv Research Agent", page_icon="üìö")
st.title("üìö Arxiv Research Agent")

# 1. Caching Resource cho Index (D·ªØ li·ªáu n·∫∑ng)
@st.cache_resource
def load_index_data():
    """Ch·ªâ load d·ªØ li·ªáu Index 1 l·∫ßn ƒë·ªÉ ti·∫øt ki·ªám RAM"""
    try:
        # S·ª≠ d·ª•ng embed_model t·ª´ constants (ƒë√£ fix dimension 768)
        index_manager = IndexManagerPinecone(embed_model, "arxiv-research")
        index = index_manager.retrieve_index()
        return index
    except Exception as e:
        print(f"Index load error: {e}")
        return None

def create_agent(index, memory):
    """T·∫°o Agent m·ªõi m·ªói l·∫ßn run ƒë·ªÉ g·∫Øn ƒë√∫ng Event Loop hi·ªán t·∫°i"""
    # Fix l·ªói Loop: T·∫°o m·ªõi LLM instance ngay t·∫°i ƒë√¢y thay v√¨ import t·ª´ constants
    llm_model = Gemini(
        api_key=GOOGLE_API_KEY, 
        model_name="models/gemini-2.5-flash", 
        max_tokens=8192
    )
    # Truy·ªÅn memory t·ª´ session state v√†o
    return Agent(index, llm_model, memory=memory)

# 2. Kh·ªüi t·∫°o State ban ƒë·∫ßu
if "messages" not in st.session_state:
    st.session_state.messages = []

# L∆∞u tr·ªØ Chat Memory ri√™ng bi·ªát (kh√¥ng ph·ª• thu·ªôc Agent object)
if "chat_memory" not in st.session_state:
    st.session_state.chat_memory = ChatMemoryBuffer.from_defaults(token_limit=20000)

# --- SIDEBAR: QU·∫¢N L√ù D·ªÆ LI·ªÜU ---
with st.sidebar:
    st.header("üìÇ N·∫°p T√†i Li·ªáu (PDF)")
    st.write("T·∫£i file PDF l√™n ƒë·ªÉ d·∫°y cho AI:")
    
    uploaded_files = st.file_uploader(
        "Ch·ªçn file PDF", 
        type=['pdf'], 
        accept_multiple_files=True
    )
    
    if uploaded_files and st.button("N·∫°p v√†o Tr√≠ Tu·ªá"):
        with st.spinner("ƒêang ƒë·ªçc v√† h·ªçc t√†i li·ªáu... (C·ª© b√¨nh tƒ©nh nh√©)"):
            import os
            
            # 1. L∆∞u file t·∫°m v√†o ·ªï c·ª©ng ƒë·ªÉ th∆∞ vi·ªán ƒë·ªçc ƒë∆∞·ª£c
            temp_dir = "temp_uploads"
            os.makedirs(temp_dir, exist_ok=True)
            saved_paths = []
            
            for uploaded_file in uploaded_files:
                file_path = os.path.join(temp_dir, uploaded_file.name)
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                saved_paths.append(file_path)
            
            # 2. G·ªçi IndexManagerPinecone ƒë·ªÉ x·ª≠ l√Ω
            try:
                # T·∫°o manager m·ªõi ƒë·ªÉ x·ª≠ l√Ω upload
                # L∆∞u √Ω: L√∫c n√†y h√†m kh·ªüi t·∫°o s·∫Ω t·∫°o connection t·ªõi Pinecone
                idx_manager = IndexManagerPinecone(embed_model, "arxiv-research")
                success, msg = idx_manager.ingest_uploaded_files(saved_paths)
                
                if success:
                    st.success(f"‚úÖ {msg}")
                    # Clear index cache ƒë·ªÉ l·∫ßn load sau n√≥ c·∫≠p nh·∫≠t d·ªØ li·ªáu m·ªõi n·∫øu c·∫ßn
                    # Tuy nhi√™n Pinecone l√† vector store r·ªùi, n√™n query engine s·∫Ω t·ª± t√¨m th·∫•y data m·ªõi.
                else:
                    st.error(f"‚ùå {msg}")
                    
            except Exception as e:
                st.error(f"L·ªói khi x·ª≠ l√Ω: {e}")
            
            # 3. D·ªçn d·∫πp file t·∫°m
            for p in saved_paths:
                if os.path.exists(p):
                    os.remove(p)
    
    st.divider()
    
    # N√∫t X√≥a L·ªãch S·ª≠ Chat
    if st.button("üóëÔ∏è X√≥a L·ªãch S·ª≠ Chat", type="primary"):
        st.session_state.messages = []
        st.session_state.chat_memory.reset()
        st.rerun()

# 3. Load Index v√† T·∫°o Agent cho Run hi·ªán t·∫°i
index = load_index_data()

if index:
    # QUAN TR·ªåNG: Lu√¥n t·∫°o Agent m·ªõi cho m·ªói l·∫ßn ch·∫°y script
    # Nh∆∞ng d√πng l·∫°i b·ªô nh·ªõ c≈© (st.session_state.chat_memory)
    agent = create_agent(index, st.session_state.chat_memory)
else:
    st.error("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y Index! H√£y ch·∫°y file 'build_index.ipynb' ƒë·ªÉ t·∫°o d·ªØ li·ªáu tr∆∞·ªõc.")
    st.stop()

# 4. Hi·ªÉn th·ªã l·ªãch s·ª≠ chat UI
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# 5. X·ª≠ l√Ω Input
# C√∫ ph√°p walrus (:=) gi√∫p g√°n gi√° tr·ªã v√† ki·ªÉm tra ƒëi·ªÅu ki·ªán c√πng l√∫c
if prompt := st.chat_input("Ask me anything about research papers!"):
    # Hi·ªÉn th·ªã c√¢u h·ªèi User
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

        # X·ª≠ l√Ω c√¢u tr·∫£ l·ªùi Assistant
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            try:
                # G·ªåI ASYNC TR·ª∞C TI·∫æP QUA asyncio.run() ƒê√É PATCH
                # V√¨ nest_asyncio.apply() ƒë√£ ƒë∆∞·ª£c g·ªçi ·ªü ƒë·∫ßu, ta c√≥ th·ªÉ d√πng loop.run_until_complete an to√†n
                # Ho·∫∑c ƒë∆°n gi·∫£n h∆°n: g·ªçi th·∫≥ng h√†m chat (b√™n trong agent class ƒë√£ c√≥ c∆° ch·∫ø g·ªçi)
                
                # C√°ch 1: G·ªçi qua event loop hi·ªán t·∫°i (An to√†n nh·∫•t v·ªõi Streamlit)
                loop = asyncio.get_event_loop()
                answer_text = loop.run_until_complete(agent.chat(prompt))
                
                st.markdown(answer_text)
                
                # L∆∞u l·ªãch s·ª≠ UI
                st.session_state.messages.append({"role": "assistant", "content": answer_text})
            except RuntimeError as e:
                # N·∫øu loop ƒë√£ ƒë√≥ng ho·∫∑c l·ªói loop
                st.error(f"Async Loop Error: {e}")
                # Fallback: T·∫°o loop m·ªõi (√≠t khi c·∫ßn nh·ªù nest_asyncio)
                new_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(new_loop)
                answer_text = new_loop.run_until_complete(agent.chat(prompt))
                st.markdown(answer_text)
                st.session_state.messages.append({"role": "assistant", "content": answer_text})
            except Exception as e:
                import traceback
                st.error(f"Error details: {traceback.format_exc()}")
