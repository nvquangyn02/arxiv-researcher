{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f00dec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import fetch_arxiv_papers\n",
    "\n",
    "# 1. T√¨m ki·∫øm v√† t·∫£i th√¥ng tin b√†i b√°o\n",
    "# H√†m n√†y l√™n Arxiv t√¨m c√°c b√†i c√≥ t·ª´ kh√≥a \"Language Models\" (l·∫•y 10 b√†i m·ªõi nh·∫•t)\n",
    "papers = fetch_arxiv_papers(\"Language Models\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2065991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Evaluation of Oncotimia: An LLM based system for supporting tumour boards',\n",
       " 'Post-LayerNorm Is Back: Stable, ExpressivE, and Deep',\n",
       " 'Reflective Translation: Improving Low-Resource Machine Translation via Structured Self-Reflection',\n",
       " 'EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning',\n",
       " 'Identifying and Transferring Reasoning-Critical Neurons: Improving LLM Inference Reliability via Activation Steering',\n",
       " 'HARMONI: Multimodal Personalization of Multi-User Human-Robot Interactions with LLMs',\n",
       " 'Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models',\n",
       " 'Neural Neural Scaling Laws',\n",
       " 'When Iterative RAG Beats Ideal Evidence: A Diagnostic Study in Scientific Multi-hop Question Answering',\n",
       " 'Zero-Shot Stance Detection in the Wild: Dynamic Target Generation and Multi-Target Adaptation']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Ki·ªÉm tra nhanh danh s√°ch ti√™u ƒë·ªÅ\n",
    "# In ra list ti√™u ƒë·ªÅ c·ªßa c√°c b√†i v·ª´a t·∫£i ƒë·ªÉ xem c√≥ ƒë√∫ng ch·ªß ƒë·ªÅ kh√¥ng\n",
    "[paper[\"title\"] for paper in papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1d16de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "# 3. H√†m chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√¥ sang Document\n",
    "# Convert t·ª´ ƒëi·ªÉn (dict) ch·ª©a th√¥ng tin b√†i b√°o sang ƒë·ªëi t∆∞·ª£ng 'Document' c·ªßa LlamaIndex\n",
    "# ƒë·ªÉ chu·∫©n b·ªã cho vi·ªác nh√∫ng (embedding)\n",
    "def create_documents_from_papers(papers):\n",
    "    documents = []\n",
    "    for paper in papers:\n",
    "        # Gom c√°c th√¥ng tin quan tr·ªçng v√†o m·ªôt chu·ªói vƒÉn b·∫£n duy nh·∫•t\n",
    "        content = (\n",
    "            f\"Title: {paper['title']}\\n\"\n",
    "            f\"Authors: {', '.join(paper['authors'])}\\n\"\n",
    "            f\"Abstract: {paper['summary']}\\n\"\n",
    "            f\"URL: {paper.get('pdf_url', 'N/A')}\\n\"\n",
    "            f\"DOI: {paper.get('doi', 'N/A')}\\n\"\n",
    "            f\"Primary Category: {paper['primary_category']}\\n\"\n",
    "            f\"arXiv URL: {paper['arxiv_url']}\\n\"\n",
    "        )\n",
    "        documents.append(Document(text=content))\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed2d95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Th·ª±c hi·ªán chuy·ªÉn ƒë·ªïi\n",
    "documents = create_documents_from_papers(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92a7f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='219fc049-ca88-433a-b064-f0e518fa4dff', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Evaluation of Oncotimia: An LLM based system for supporting tumour boards\\nAuthors: Luis Lorenzo, Marcos Montana-Mendez, Sergio Figueiras, Miguel Boubeta, Cristobal Bernardo-Castineira\\nAbstract: Multidisciplinary tumour boards (MDTBs) play a central role in oncology decision-making but require manual processes and structuring large volumes of heterogeneous clinical information, resulting in a substantial documentation burden. In this work, we present ONCOTIMIA, a modular and secure clinical tool designed to integrate generative artificial intelligence (GenAI) into oncology workflows and evaluate its application to the automatic completion of lung cancer tumour board forms using large language models (LLMs). The system combines a multi-layer data lake, hybrid relational and vector storage, retrieval-augmented generation (RAG) and a rule-driven adaptive form model to transform unstructured clinical documentation into structured and standardised tumour board records. We assess the performance of six LLMs deployed through AWS Bedrock on ten lung cancer cases, measuring both completion form accuracy and end-to-end latency. The results demonstrate high performance across models, with the best performing configuration achieving an 80% of correct field completion and clinically acceptable response time for most LLMs. Larger and more recent models exhibit best accuracies without incurring prohibitive latency. These findings provide empirical evidence that LLM- assisted autocompletion form is technically feasible and operationally viable in multidisciplinary lung cancer workflows and support its potential to significantly reduce documentation burden while preserving data quality.\\nURL: https://arxiv.org/pdf/2601.19899v1\\nDOI: None\\nPrimary Category: cs.CL\\narXiv URL: http://arxiv.org/abs/2601.19899v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ecded120-35bc-4951-ba8c-0b1d0c78cf10', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Post-LayerNorm Is Back: Stable, ExpressivE, and Deep\\nAuthors: Chen Chen, Lai Wei\\nAbstract: Large language model (LLM) scaling is hitting a wall. Widening models yields diminishing returns, and extending context length does not improve fundamental expressivity. In contrast, depth scaling offers theoretically superior expressivity, yet current Transformer architectures struggle to train reliably at extreme depths. We revisit the Post-LayerNorm (Post-LN) formulation, whose instability at scale caused its replacement by Pre-LN in modern LLMs. We show that the central failure mode of Post-LN arises from the ResNet-style residual pathway, which introduces gradient vanishing in deep networks. We present Keel, a Post-LN Transformer that replaces this residual path with a Highway-style connection. This modification preserves the gradient flow through the residual branch, preventing signal vanishing from the top layers to the bottom. Unlike prior methods, Keel enables stable training at extreme depths without requiring specialized initialization or complex optimization tricks. Keel trains robustly at depths exceeding 1000 layers and consistently improves perplexity and depth-scaling characteristics over Pre-LN. These findings indicate that Post-LN, when paired with a Highway-style connection, provides a simple and effective foundation for building deeply scalable LLMs, opening the possibility for future infinite-depth architectures.\\nURL: https://arxiv.org/pdf/2601.19895v1\\nDOI: None\\nPrimary Category: cs.LG\\narXiv URL: http://arxiv.org/abs/2601.19895v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='db1a2044-a0ba-4a96-8f1e-40dd10c2e1e9', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Reflective Translation: Improving Low-Resource Machine Translation via Structured Self-Reflection\\nAuthors: Nicholas Cheng\\nAbstract: Low-resource languages such as isiZulu and isiXhosa face persistent challenges in machine translation due to limited parallel data and linguistic resources. Recent advances in large language models suggest that self-reflection, prompting a model to critique and revise its own outputs, can improve reasoning quality and factual consistency. Building on this idea, this paper introduces Reflective Translation, a prompt-based framework in which a model generates an initial translation, produces a structured self-critique, and then uses this reflection to generate a refined translation. The approach is evaluated on English-isiZulu and English-isiXhosa translation using OPUS-100 and NTREX-African, across multiple prompting strategies and confidence thresholds. Results show consistent improvements in both BLEU and COMET scores between first- and second-pass translations, with average gains of up to +0.22 BLEU and +0.18 COMET. Statistical significance testing using paired nonparametric tests confirms that these improvements are robust. The proposed method is model-agnostic, requires no fine-tuning, and introduces a reflection-augmented dataset that can support future supervised or analysis-driven work. These findings demonstrate that structured self-reflection is a practical and effective mechanism for improving translation quality in low-resource settings.\\nURL: https://arxiv.org/pdf/2601.19871v1\\nDOI: None\\nPrimary Category: cs.CL\\narXiv URL: http://arxiv.org/abs/2601.19871v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='db816cb7-d76c-47cb-b6a8-ba292a30fb32', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning\\nAuthors: Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu, Muzammal Naseer, Chi-Wing Fu, Pheng-Ann Heng\\nAbstract: Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: https://github.com/Nicous20/EgoHandICL\\nURL: https://arxiv.org/pdf/2601.19850v1\\nDOI: None\\nPrimary Category: cs.CV\\narXiv URL: http://arxiv.org/abs/2601.19850v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c450809f-f263-44f0-a48f-37dc01ec2583', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Identifying and Transferring Reasoning-Critical Neurons: Improving LLM Inference Reliability via Activation Steering\\nAuthors: Fangan Dong, Zuming Yan, Xuri Ge, Zhiwei Xu, Mengqi Zhang, Xuanang Chen, Ben He, Xin Xin, Zhumin Chen, Ying Zhou\\nAbstract: Despite the strong reasoning capabilities of recent large language models (LLMs), achieving reliable performance on challenging tasks often requires post-training or computationally expensive sampling strategies, limiting their practical efficiency. In this work, we first show that a small subset of neurons in LLMs exhibits strong predictive correlations with reasoning correctness. Based on this observation, we propose AdaRAS (Adaptive Reasoning Activation Steering), a lightweight test-time framework that improves reasoning reliability by selectively intervening on neuron activations. AdaRAS identifies Reasoning-Critical Neurons (RCNs) via a polarity-aware mean-difference criterion and adaptively steers their activations during inference, enhancing incorrect reasoning traces while avoiding degradation on already-correct cases. Experiments on 10 mathematics and coding benchmarks demonstrate consistent improvements, including over 13% gains on AIME-24 and AIME-25. Moreover, AdaRAS exhibits strong transferability across datasets and scalability to stronger models, outperforming post-training methods without additional training or sampling cost.\\nURL: https://arxiv.org/pdf/2601.19847v1\\nDOI: None\\nPrimary Category: cs.CL\\narXiv URL: http://arxiv.org/abs/2601.19847v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='5a17fd10-1990-4efa-a437-83805142d35e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: HARMONI: Multimodal Personalization of Multi-User Human-Robot Interactions with LLMs\\nAuthors: Jeanne Mal√©cot, Hamed Rahimi, Jeanne Cattoni, Marie Samson, Mouad Abrini, Mahdi Khoramshahi, Maribel Pino, Mohamed Chetouani\\nAbstract: Existing human-robot interaction systems often lack mechanisms for sustained personalization and dynamic adaptation in multi-user environments, limiting their effectiveness in real-world deployments. We present HARMONI, a multimodal personalization framework that leverages large language models to enable socially assistive robots to manage long-term multi-user interactions. The framework integrates four key modules: (i) a perception module that identifies active speakers and extracts multimodal input; (ii) a world modeling module that maintains representations of the environment and short-term conversational context; (iii) a user modeling module that updates long-term speaker-specific profiles; and (iv) a generation module that produces contextually grounded and ethically informed responses. Through extensive evaluation and ablation studies on four datasets, as well as a real-world scenario-driven user-study in a nursing home environment, we demonstrate that HARMONI supports robust speaker identification, online memory updating, and ethically aligned personalization, outperforming baseline LLM-driven approaches in user modeling accuracy, personalization quality, and user satisfaction.\\nURL: https://arxiv.org/pdf/2601.19839v1\\nDOI: None\\nPrimary Category: cs.RO\\narXiv URL: http://arxiv.org/abs/2601.19839v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='116074be-00a4-4d94-84e0-1f99ee121507', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Visual Generation Unlocks Human-Like Reasoning through Multimodal World Models\\nAuthors: Jialong Wu, Xiaoying Zhang, Hongyi Yuan, Xiangcheng Zhang, Tianhao Huang, Changjing He, Chaoyi Deng, Renrui Zhang, Youbin Wu, Mingsheng Long\\nAbstract: Humans construct internal world models and reason by manipulating the concepts within these models. Recent advances in AI, particularly chain-of-thought (CoT) reasoning, approximate such human cognitive abilities, where world models are believed to be embedded within large language models. Expert-level performance in formal and abstract domains such as mathematics and programming has been achieved in current systems by relying predominantly on verbal reasoning. However, they still lag far behind humans in domains like physical and spatial intelligence, which require richer representations and prior knowledge. The emergence of unified multimodal models (UMMs) capable of both verbal and visual generation has therefore sparked interest in more human-like reasoning grounded in complementary multimodal pathways, though their benefits remain unclear. From a world-model perspective, this paper presents the first principled study of when and how visual generation benefits reasoning. Our key position is the visual superiority hypothesis: for certain tasks--particularly those grounded in the physical world--visual generation more naturally serves as world models, whereas purely verbal world models encounter bottlenecks arising from representational limitations or insufficient prior knowledge. Theoretically, we formalize internal world modeling as a core component of CoT reasoning and analyze distinctions among different forms of world models. Empirically, we identify tasks that necessitate interleaved visual-verbal CoT reasoning, constructing a new evaluation suite, VisWorld-Eval. Controlled experiments on a state-of-the-art UMM show that interleaved CoT significantly outperforms purely verbal CoT on tasks that favor visual world modeling, but offers no clear advantage otherwise. Together, this work clarifies the potential of multimodal world modeling for more powerful, human-like multimodal AI.\\nURL: https://arxiv.org/pdf/2601.19834v1\\nDOI: None\\nPrimary Category: cs.AI\\narXiv URL: http://arxiv.org/abs/2601.19834v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b6b30adc-0207-4696-9844-bbe5b1fdfd6e', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Neural Neural Scaling Laws\\nAuthors: Michael Y. Hu, Jane Pan, Ayush Rajesh Jhaveri, Nicholas Lourie, Kyunghyun Cho\\nAbstract: Neural scaling laws predict how language model performance improves with increased compute. While aggregate metrics like validation loss can follow smooth power-law curves, individual downstream tasks exhibit diverse scaling behaviors: some improve monotonically, others plateau, and some even degrade with scale. We argue that predicting downstream performance from validation perplexity suffers from two limitations: averaging token-level losses obscures signal, and no simple parametric family can capture the full spectrum of scaling behaviors. To address this, we propose Neural Neural Scaling Laws (NeuNeu), a neural network that frames scaling law prediction as time-series extrapolation. NeuNeu combines temporal context from observed accuracy trajectories with token-level validation losses, learning to predict future performance without assuming any bottleneck or functional form. Trained entirely on open-source model checkpoints from HuggingFace, NeuNeu achieves 2.04% mean absolute error in predicting model accuracy on 66 downstream tasks -- a 38% reduction compared to logistic scaling laws (3.29% MAE). Furthermore, NeuNeu generalizes zero-shot to unseen model families, parameter counts, and downstream tasks. Our work suggests that predicting downstream scaling laws directly from data outperforms parametric alternatives.\\nURL: https://arxiv.org/pdf/2601.19831v1\\nDOI: None\\nPrimary Category: cs.LG\\narXiv URL: http://arxiv.org/abs/2601.19831v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8bf6b050-7505-4e04-bc54-8e782ed673c5', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: When Iterative RAG Beats Ideal Evidence: A Diagnostic Study in Scientific Multi-hop Question Answering\\nAuthors: Mahdi Astaraki, Mohammad Arshi Saloot, Ali Shiraee Kasmaee, Hamidreza Mahyar, Soheila Samiee\\nAbstract: Retrieval-Augmented Generation (RAG) extends large language models (LLMs) beyond parametric knowledge, yet it is unclear when iterative retrieval-reasoning loops meaningfully outperform static RAG, particularly in scientific domains with multi-hop reasoning, sparse domain knowledge, and heterogeneous evidence. We provide the first controlled, mechanism-level diagnostic study of whether synchronized iterative retrieval and reasoning can surpass an idealized static upper bound (Gold Context) RAG. We benchmark eleven state-of-the-art LLMs under three regimes: (i) No Context, measuring reliance on parametric memory; (ii) Gold Context, where all oracle evidence is supplied at once; and (iii) Iterative RAG, a training-free controller that alternates retrieval, hypothesis refinement, and evidence-aware stopping. Using the chemistry-focused ChemKGMultiHopQA dataset, we isolate questions requiring genuine retrieval and analyze behavior with diagnostics spanning retrieval coverage gaps, anchor-carry drop, query quality, composition fidelity, and control calibration. Across models, Iterative RAG consistently outperforms Gold Context, with gains up to 25.6 percentage points, especially for non-reasoning fine-tuned models. Staged retrieval reduces late-hop failures, mitigates context overload, and enables dynamic correction of early hypothesis drift, but remaining failure modes include incomplete hop coverage, distractor latch trajectories, early stopping miscalibration, and high composition failure rates even with perfect retrieval. Overall, staged retrieval is often more influential than the mere presence of ideal evidence; we provide practical guidance for deploying and diagnosing RAG systems in specialized scientific settings and a foundation for more reliable, controllable iterative retrieval-reasoning frameworks.\\nURL: https://arxiv.org/pdf/2601.19827v1\\nDOI: None\\nPrimary Category: cs.CL\\narXiv URL: http://arxiv.org/abs/2601.19827v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f11f6add-3208-4fd6-b32c-f985490db854', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Title: Zero-Shot Stance Detection in the Wild: Dynamic Target Generation and Multi-Target Adaptation\\nAuthors: Aohua Li, Yuanshuo Zhang, Ge Gao, Bo Chen, Xiaobing Zhao\\nAbstract: Current stance detection research typically relies on predicting stance based on given targets and text. However, in real-world social media scenarios, targets are neither predefined nor static but rather complex and dynamic. To address this challenge, we propose a novel task: zero-shot stance detection in the wild with Dynamic Target Generation and Multi-Target Adaptation (DGTA), which aims to automatically identify multiple target-stance pairs from text without prior target knowledge. We construct a Chinese social media stance detection dataset and design multi-dimensional evaluation metrics. We explore both integrated and two-stage fine-tuning strategies for large language models (LLMs) and evaluate various baseline models. Experimental results demonstrate that fine-tuned LLMs achieve superior performance on this task: the two-stage fine-tuned Qwen2.5-7B attains the highest comprehensive target recognition score of 66.99%, while the integrated fine-tuned DeepSeek-R1-Distill-Qwen-7B achieves a stance detection F1 score of 79.26%.\\nURL: https://arxiv.org/pdf/2601.19802v1\\nDOI: None\\nPrimary Category: cs.CL\\narXiv URL: http://arxiv.org/abs/2601.19802v1\\n', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Xem th·ª≠ d·ªØ li·ªáu Document\n",
    "# In ra danh s√°ch c√°c document ƒë√£ t·∫°o ƒë·ªÉ ki·ªÉm tra tr∆∞·ªõc khi Index\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0043d184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang d√πng embed model: models/gemini-embedding-001\n",
      "‚úÖ T·∫°o Index th√†nh c√¥ng!\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings, VectorStoreIndex \n",
    "import constants\n",
    "import importlib\n",
    "# Reload constants ƒë·ªÉ ƒë·∫£m b·∫£o nh·∫≠n config m·ªõi nh·∫•t (tr√°nh l·ªói cache notebook)\n",
    "importlib.reload(constants)\n",
    "from constants import embed_model \n",
    "\n",
    "# 6. C·∫•u h√¨nh c·∫Øt nh·ªè d·ªØ li·ªáu (Chunking)\n",
    "# Chia vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n nh·ªè 1024 token, g·ªëi ƒë·∫ßu nhau 50 token\n",
    "Settings.chunk_size = 1024\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "# In ra t√™n model ƒë·ªÉ ki·ªÉm tra ch·∫Øc ch·∫Øn\n",
    "print(f\"ƒêang d√πng embed model: {embed_model.model_name}\")\n",
    "\n",
    "# 7. X√¢y d·ª±ng Vector Index (QUAN TR·ªåNG)\n",
    "# B∆∞·ªõc n√†y s·∫Ω g·ª≠i d·ªØ li·ªáu l√™n Google Gemini Embedding ƒë·ªÉ chuy·ªÉn th√†nh vector s·ªë\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model) \n",
    "print(\"‚úÖ T·∫°o Index th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d2d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. L∆∞u Index xu·ªëng ·ªï c·ª©ng\n",
    "# L∆∞u v√†o th∆∞ m·ª•c \"index/\" ƒë·ªÉ sau n√†y d√πng l·∫°i (b√™n file agent.ipynb) m√† kh√¥ng c·∫ßn build l·∫°i\n",
    "index.storage_context.persist(persist_dir=\"index/\")\n",
    "print(\"üíæ ƒê√£ l∆∞u Index v√†o th∆∞ m·ª•c 'index/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4ced5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C√¢u tr·∫£ l·ªùi:\n",
      "The papers discuss two distinct topics within the field of natural language processing. One paper focuses on the effectiveness of iterative Retrieval-Augmented Generation (RAG) in scientific multi-hop question answering, comparing its performance against static RAG with ideal evidence and analyzing its failure modes. The other paper explores the use of structured self-reflection to enhance machine translation quality, particularly for low-resource languages, by enabling models to critique and revise their own translations.\n"
     ]
    }
   ],
   "source": [
    "# 9. Test th·ª≠ h·ªèi ƒë√°p (RAG)\n",
    "# Th·ª≠ h·ªèi model m·ªôt c√¢u d·ª±a tr√™n d·ªØ li·ªáu v·ª´a index ƒë·ªÉ xem n√≥ kh√¥n ƒë·∫øn ƒë√¢u\n",
    "from constants import llm_model\n",
    "\n",
    "# T·∫°o query engine v·ªõi LLM x·ªãn x√≤ (Gemini 2.5 Flash)\n",
    "query_engine = index.as_query_engine(llm=llm_model)\n",
    "\n",
    "# ƒê·∫∑t c√¢u h·ªèi\n",
    "response = query_engine.query(\"What are the main topics discussed in these papers?\")\n",
    "print(f\"C√¢u tr·∫£ l·ªùi:\\n{response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
